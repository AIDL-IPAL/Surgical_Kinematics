{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9707681,
          "sourceType": "datasetVersion",
          "datasetId": 5937379
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Segmentation with Text Prompt: GroundingDINO+SAM2",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation with Text Prompt: GroundingDINO + SAM2\n",
        "\n",
        "This notebook shows how to integrate GroundingDINO and SAM2 to segment objects from an image using text prompts.\n",
        "\n",
        "**Segment Anything Model 2 (SAM2)** is a method that performs segmentation on images (as well as videos) based on prompts. The prompts here includes coordinate points or bounding boxes of interested objects, which should be manually defined. That means, if I want to segment a cat from an image, I need to either know the coordinate points of the cat in the image, or know the bounding box of the cat. Can we automate this process of finding the coordinate points or bounding boxes?\n",
        "\n",
        "**GroundingDINO** is an object detector that detects objects in an image based on text prompts. That means, we can use natural language to tell the detector what objects we want to find, and the detector will output bounding boxes of our interest objects. It does not perform segmentation, however, it only performs object detection. Thus, why not applying SAM2 after the detector has identified the bounding boxes of our interested objects?\n",
        "\n",
        "Let's explore how we can combine these two methods to segment our interested objects using text prompts.\n",
        "\n",
        "### <font color='289C4E'>Table of contents<font><a class='anchor' id='top'></a>\n",
        "- [Setup and Installation](#setup)\n",
        "- [Import Libraries](#import)\n",
        "- [Object Localization with Text Prompt](#localization)\n",
        "- [Object Segmentation with Boxes Prompt](#segmentation)\n",
        "- [Putting GroundingDINO And SAM2 Together](#together)\n",
        "- [More Challenging Text Prompt](#challenging)\n",
        "    \n",
        "Reference:\n",
        "- Github: https://github.com/IDEA-Research/GroundingDINO, https://github.com/facebookresearch/sam2\n",
        "- Papers: https://arxiv.org/pdf/2303.05499, https://arxiv.org/abs/2408.00714"
      ],
      "metadata": {
        "id": "VtB0BKqT2FdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Installation <a class='anchor' id='setup'></a> [‚Üë](#top)\n",
        "\n",
        "First, we need to setup the environment and install the required dependencies for both GroundingDINO and SAM2.\n",
        "\n",
        "Before that, we need to check if the environment variable `CUDA_HOME` has been set by using the command following command."
      ],
      "metadata": {
        "id": "0mQyCPaX2FdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $CUDA_HOME"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:17:36.016434Z",
          "iopub.execute_input": "2024-10-31T04:17:36.016771Z",
          "iopub.status.idle": "2024-10-31T04:17:37.021389Z",
          "shell.execute_reply.started": "2024-10-31T04:17:36.016736Z",
          "shell.execute_reply": "2024-10-31T04:17:37.020304Z"
        },
        "trusted": true,
        "id": "5jPgciey2FdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the above command prints nothing, that means the variable has not been set. We then need to check the path to the cuda using command `!which nvcc`, and then set the variable using `!export CUDA_HOME=<path-to-cuda>`, where `<path-to-cuda>` is the output of `!which nvcc`.\n",
        "\n",
        "Now we can clone the GroundingDINO Github repository and install the required dependencies."
      ],
      "metadata": {
        "id": "WkbrX5IW2FdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd GroundingDINO/\n",
        "!pip install -q -e ."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:17:37.023692Z",
          "iopub.execute_input": "2024-10-31T04:17:37.024082Z",
          "iopub.status.idle": "2024-10-31T04:18:36.555745Z",
          "shell.execute_reply.started": "2024-10-31T04:17:37.024036Z",
          "shell.execute_reply": "2024-10-31T04:18:36.554561Z"
        },
        "trusted": true,
        "id": "Nd15VK_Y2FdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now download the GroundingDINO pre-trained model's weights to be used for our inference (or prediction) later."
      ],
      "metadata": {
        "id": "JnXOmXT52FdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir weights\n",
        "%cd weights\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "%cd .."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:18:36.557475Z",
          "iopub.execute_input": "2024-10-31T04:18:36.557916Z",
          "iopub.status.idle": "2024-10-31T04:18:47.702114Z",
          "shell.execute_reply.started": "2024-10-31T04:18:36.557867Z",
          "shell.execute_reply": "2024-10-31T04:18:47.700686Z"
        },
        "trusted": true,
        "id": "yqnZM_2U2FdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, for SAM2, we can clone its Github repository and install its dependencies."
      ],
      "metadata": {
        "id": "BBPRcj6D2FdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "!git clone https://github.com/facebookresearch/sam2.git\n",
        "%cd sam2\n",
        "!pip install -q -e ."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:18:47.704836Z",
          "iopub.execute_input": "2024-10-31T04:18:47.705175Z",
          "iopub.status.idle": "2024-10-31T04:21:40.318599Z",
          "shell.execute_reply.started": "2024-10-31T04:18:47.705139Z",
          "shell.execute_reply": "2024-10-31T04:21:40.317349Z"
        },
        "trusted": true,
        "id": "BSCRJS8x2FdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to download the pre-trained SAM2 model checkpoints."
      ],
      "metadata": {
        "id": "TjQHAorJ2FdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd sam2\n",
        "!mkdir -p checkpoints/\n",
        "!wget -P checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "%cd .."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:21:40.320282Z",
          "iopub.execute_input": "2024-10-31T04:21:40.320632Z",
          "iopub.status.idle": "2024-10-31T04:21:45.485705Z",
          "shell.execute_reply.started": "2024-10-31T04:21:40.320593Z",
          "shell.execute_reply": "2024-10-31T04:21:45.484573Z"
        },
        "trusted": true,
        "id": "AuWZ_Qar2FdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:21:45.487137Z",
          "iopub.execute_input": "2024-10-31T04:21:45.487516Z",
          "iopub.status.idle": "2024-10-31T04:21:45.493482Z",
          "shell.execute_reply.started": "2024-10-31T04:21:45.487479Z",
          "shell.execute_reply": "2024-10-31T04:21:45.492571Z"
        },
        "trusted": true,
        "id": "rYhy0ruU2FdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries <a class='anchor' id='import'></a> [‚Üë](#top)\n",
        "\n",
        "After finishing the environment setup and installation, we can import some required libraries."
      ],
      "metadata": {
        "id": "aOk-Q-gX2FdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working/GroundingDINO\n",
        "from groundingdino.util.inference import load_model, load_image, predict, annotate"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:21:45.494618Z",
          "iopub.execute_input": "2024-10-31T04:21:45.494905Z",
          "iopub.status.idle": "2024-10-31T04:21:51.699396Z",
          "shell.execute_reply.started": "2024-10-31T04:21:45.494875Z",
          "shell.execute_reply": "2024-10-31T04:21:51.698392Z"
        },
        "trusted": true,
        "id": "Ci07JZ3p2FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working/sam2\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:21:51.700605Z",
          "iopub.execute_input": "2024-10-31T04:21:51.70098Z",
          "iopub.status.idle": "2024-10-31T04:21:51.940523Z",
          "shell.execute_reply.started": "2024-10-31T04:21:51.700937Z",
          "shell.execute_reply": "2024-10-31T04:21:51.939544Z"
        },
        "trusted": true,
        "id": "u6iYg7j62FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working/\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "from torchvision.ops import box_convert\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:21:51.941756Z",
          "iopub.execute_input": "2024-10-31T04:21:51.942062Z",
          "iopub.status.idle": "2024-10-31T04:21:51.950618Z",
          "shell.execute_reply.started": "2024-10-31T04:21:51.942031Z",
          "shell.execute_reply": "2024-10-31T04:21:51.948814Z"
        },
        "trusted": true,
        "id": "epLOhl1v2FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Localization with Text Prompt <a class='anchor' id='localization'></a> [‚Üë](#top)\n",
        "\n",
        "We first explore the GroundingDINO method that takes an image and text prompt as input, and outputs a list of bounding boxes that matches the prompt.\n",
        "\n",
        "Here, we will use a pre-trained model, so we need to load it first."
      ],
      "metadata": {
        "id": "mEGPTPRe2FdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"GroundingDINO/weights/groundingdino_swint_ogc.pth\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:21:51.953975Z",
          "iopub.execute_input": "2024-10-31T04:21:51.954286Z",
          "iopub.status.idle": "2024-10-31T04:21:56.8631Z",
          "shell.execute_reply.started": "2024-10-31T04:21:51.954255Z",
          "shell.execute_reply": "2024-10-31T04:21:56.86223Z"
        },
        "trusted": true,
        "id": "aDbGfujL2FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set our input arguments. Please change the `IMAGE_PATH` to our own image, and set a `TEXT_PROMPT` that tells the objects that you would like to detect in the image. Following the suggestion from GroundingDINO Github page, we separate different category names with `.`. The text prompt can be a sentence as well, not necessary words. The `BOX_TRESHOLD` and `TEXT_TRESHOLD` set here are the default values."
      ],
      "metadata": {
        "id": "QDNkuToa2FdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/desk.jpg\"\n",
        "TEXT_PROMPT = \"book . ball . bottle .\"\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:21:56.864544Z",
          "iopub.execute_input": "2024-10-31T04:21:56.86493Z",
          "iopub.status.idle": "2024-10-31T04:21:56.870323Z",
          "shell.execute_reply.started": "2024-10-31T04:21:56.864888Z",
          "shell.execute_reply": "2024-10-31T04:21:56.869313Z"
        },
        "trusted": true,
        "id": "yzD0kmSf2FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_source, image = load_image(IMAGE_PATH)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:21:56.871808Z",
          "iopub.execute_input": "2024-10-31T04:21:56.872162Z",
          "iopub.status.idle": "2024-10-31T04:21:57.123747Z",
          "shell.execute_reply.started": "2024-10-31T04:21:56.872121Z",
          "shell.execute_reply": "2024-10-31T04:21:57.122856Z"
        },
        "trusted": true,
        "id": "LamnvNRH2FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we use the `predict()` function that we imported earlier to predict the bounding boxes based on the input prompt."
      ],
      "metadata": {
        "id": "07TCoaFw2FdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boxes, logits, phrases = predict(\n",
        "    model=model,\n",
        "    image=image,\n",
        "    caption=TEXT_PROMPT,\n",
        "    box_threshold=BOX_TRESHOLD,\n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:04.235451Z",
          "iopub.execute_input": "2024-10-31T04:23:04.235886Z",
          "iopub.status.idle": "2024-10-31T04:23:19.118267Z",
          "shell.execute_reply.started": "2024-10-31T04:23:04.235847Z",
          "shell.execute_reply": "2024-10-31T04:23:19.117306Z"
        },
        "trusted": true,
        "id": "VOCbV7IU2FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the `annotate()` function to create an image with the output bounding boxes and their respective category names and confidence scores."
      ],
      "metadata": {
        "id": "2bmAAFKm2FdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:22.404345Z",
          "iopub.execute_input": "2024-10-31T04:23:22.405464Z",
          "iopub.status.idle": "2024-10-31T04:23:22.438765Z",
          "shell.execute_reply.started": "2024-10-31T04:23:22.405422Z",
          "shell.execute_reply": "2024-10-31T04:23:22.43773Z"
        },
        "trusted": true,
        "id": "dgPk8InP2FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the results üëÄ"
      ],
      "metadata": {
        "id": "hO22POzO2FdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:26.111188Z",
          "iopub.execute_input": "2024-10-31T04:23:26.112014Z",
          "iopub.status.idle": "2024-10-31T04:23:28.082278Z",
          "shell.execute_reply.started": "2024-10-31T04:23:26.111972Z",
          "shell.execute_reply": "2024-10-31T04:23:28.081366Z"
        },
        "trusted": true,
        "id": "Hc-VhG5r2FdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the result to a specified path."
      ],
      "metadata": {
        "id": "1KbQZz6N2FdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite(\"/kaggle/working/desk-annotated_image.jpg\", annotated_frame)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:32.98525Z",
          "iopub.execute_input": "2024-10-31T04:23:32.985707Z",
          "iopub.status.idle": "2024-10-31T04:23:33.055782Z",
          "shell.execute_reply.started": "2024-10-31T04:23:32.985638Z",
          "shell.execute_reply": "2024-10-31T04:23:33.054825Z"
        },
        "trusted": true,
        "id": "s-eSVaKP2FdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have done with object detection using text prompt with GroundingDINO üòÉ But before we pass these bounding boxes to our SAM2 model, we need to do some modifications to the format of the bounding boxes so that the SAM2 can function properly.\n",
        "\n",
        "Here is the code for converting the list of bounding boxes to a format recognize by SAM2."
      ],
      "metadata": {
        "id": "DeyaFr4u2FdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h, w, _ = image_source.shape\n",
        "boxes_unnorm = boxes * torch.Tensor([w, h, w, h])\n",
        "boxes_xyxy = box_convert(boxes=boxes_unnorm, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:36.393481Z",
          "iopub.execute_input": "2024-10-31T04:23:36.394382Z",
          "iopub.status.idle": "2024-10-31T04:23:36.399642Z",
          "shell.execute_reply.started": "2024-10-31T04:23:36.394329Z",
          "shell.execute_reply": "2024-10-31T04:23:36.398711Z"
        },
        "trusted": true,
        "id": "rtDQhCv52FdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Segmentation with Boxes Prompt <a class='anchor' id='segmentation'></a> [‚Üë](#top)\n",
        "\n",
        "Now let's explore SAM2.\n",
        "\n",
        "We have obtained bounding boxes using GroundingDINO previously, and we want the SAM2 to properly segment the objects in the bounding boxes.\n",
        "\n",
        "For simplicity, we change our working path to `/kaggle/working/sam2`."
      ],
      "metadata": {
        "id": "fHUIGeEn2FdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working/sam2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:39.762144Z",
          "iopub.execute_input": "2024-10-31T04:23:39.762556Z",
          "iopub.status.idle": "2024-10-31T04:23:39.768524Z",
          "shell.execute_reply.started": "2024-10-31T04:23:39.762517Z",
          "shell.execute_reply": "2024-10-31T04:23:39.767587Z"
        },
        "trusted": true,
        "id": "unFR70yM2FdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained SAM2 model. Here we choose `hiera_large` model, so we specify its corresponding checkpoints that we downloaded earlier, and the pre-defined configuration file (this is installed together when we clone the repository). We then create a SAM2 model and a predictor to predict the segments (SAM2 also has a generator automatically generates segmentation masks for the whole image, not a specific category or class, but we won't use it here)."
      ],
      "metadata": {
        "id": "D6h0zDv72FdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sam2_checkpoint = \"sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device='cuda')\n",
        "\n",
        "predictor = SAM2ImagePredictor(sam2_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:43.096176Z",
          "iopub.execute_input": "2024-10-31T04:23:43.096975Z",
          "iopub.status.idle": "2024-10-31T04:23:46.588006Z",
          "shell.execute_reply.started": "2024-10-31T04:23:43.096934Z",
          "shell.execute_reply": "2024-10-31T04:23:46.587025Z"
        },
        "trusted": true,
        "id": "Sn71k9a42FdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some helper function responsible for plotting the results."
      ],
      "metadata": {
        "id": "wmueC98O2FdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(3)\n",
        "\n",
        "def show_mask(mask, ax, random_color=False, borders = True):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask = mask.astype(np.uint8)\n",
        "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    if borders:\n",
        "        import cv2\n",
        "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "        # Try to smooth contours\n",
        "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
        "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:50.308765Z",
          "iopub.execute_input": "2024-10-31T04:23:50.309532Z",
          "iopub.status.idle": "2024-10-31T04:23:50.319582Z",
          "shell.execute_reply.started": "2024-10-31T04:23:50.309493Z",
          "shell.execute_reply": "2024-10-31T04:23:50.318592Z"
        },
        "trusted": true,
        "id": "dWdhPWiu2FdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load our image and predict the segmentation masks. We use the same input image as above, and set the image to the SAM2 predictor using `set_image()`."
      ],
      "metadata": {
        "id": "7Ke0ppzQ2FdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('/kaggle/input/private-image/test_data/desk.jpg')\n",
        "image = np.array(image.convert(\"RGB\"))\n",
        "predictor.set_image(image)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:53.92288Z",
          "iopub.execute_input": "2024-10-31T04:23:53.923519Z",
          "iopub.status.idle": "2024-10-31T04:23:54.282619Z",
          "shell.execute_reply.started": "2024-10-31T04:23:53.92348Z",
          "shell.execute_reply": "2024-10-31T04:23:54.281614Z"
        },
        "trusted": true,
        "id": "qvyzWDLj2FdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we just call the `predict()` function and pass the bounding boxes we obtained previously (`boxes_xyxy`) as input. We will obtain the segmentation masks and their corresponding confidence scores."
      ],
      "metadata": {
        "id": "P1K7eZt92FdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, _ = predictor.predict(\n",
        "    point_coords=None,\n",
        "    point_labels=None,\n",
        "    box=boxes_xyxy,\n",
        "    multimask_output=False,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:56.519879Z",
          "iopub.execute_input": "2024-10-31T04:23:56.520811Z",
          "iopub.status.idle": "2024-10-31T04:23:56.789338Z",
          "shell.execute_reply.started": "2024-10-31T04:23:56.520766Z",
          "shell.execute_reply": "2024-10-31T04:23:56.788509Z"
        },
        "trusted": true,
        "id": "1lZScRBq2FdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the segmentation output üëÄ The following code will also save the output to `desk-segment.jpg`, change this path if necessary."
      ],
      "metadata": {
        "id": "pPHwDjFR2FdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "for mask in masks:\n",
        "    show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
        "for box in boxes_xyxy:\n",
        "    show_box(box, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.savefig(\"desk-segment.jpg\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:23:59.326781Z",
          "iopub.execute_input": "2024-10-31T04:23:59.327157Z",
          "iopub.status.idle": "2024-10-31T04:24:18.544396Z",
          "shell.execute_reply.started": "2024-10-31T04:23:59.327123Z",
          "shell.execute_reply": "2024-10-31T04:24:18.543458Z"
        },
        "trusted": true,
        "id": "Wy3ODflk2FdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting GroundingDINO And SAM2 Together <a class='anchor' id='together'></a> [‚Üë](#top)\n",
        "\n",
        "This section chains GroundingDINO and SAM2 together in a single function `segment_with_prompt()`. Everything is the same as what we did in previous sections, but we now put everything in a single function so that we can perform segmentation on different images by simply calling this function üòé\n",
        "\n",
        "Before running the codes below, **make sure you have properly [install all required packages / dependencies](#setup) and [import all the necessary libraries](#import)**.\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">  \n",
        "<b>For simplicity, we fix our working path at /kaggle/working/sam2.</b>\n",
        "If this is not your current working path, run this command: %cd /kaggle/working/sam2.\n",
        "</div>\n",
        "\n",
        "The following helper functions `show_mask` and `show_box` are exactly the same as in the previous section, but let us just define it again."
      ],
      "metadata": {
        "id": "cDDXbde32FdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(3)\n",
        "\n",
        "def show_mask(mask, ax, random_color=False, borders = True):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask = mask.astype(np.uint8)\n",
        "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    if borders:\n",
        "        import cv2\n",
        "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "        # Try to smooth contours\n",
        "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
        "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:24:28.175987Z",
          "iopub.execute_input": "2024-10-31T04:24:28.176848Z",
          "iopub.status.idle": "2024-10-31T04:24:28.187541Z",
          "shell.execute_reply.started": "2024-10-31T04:24:28.176804Z",
          "shell.execute_reply": "2024-10-31T04:24:28.186644Z"
        },
        "trusted": true,
        "id": "59WYM2yl2FdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we load the pre-trained GroundingDINO model and SAM2 model using their configuration files and checkpoints / weights. We also initialize a predictor for the SAM2 model.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "If you would like to use a different model configurations or checkpoints, feel free to change the corresponding paths.\n",
        "</div>\n",
        "\n",
        "*Check the available checkpoints for [GroundingDINO model](https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file#luggage-checkpoints) and [SAM2 model](https://github.com/facebookresearch/sam2/#download-checkpoints).*"
      ],
      "metadata": {
        "id": "epAq_ugQ2FdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"../GroundingDINO/weights/groundingdino_swint_ogc.pth\")\n",
        "sam2_model = build_sam2(\"configs/sam2.1/sam2.1_hiera_l.yaml\", \"sam2/checkpoints/sam2.1_hiera_large.pt\", device='cuda')\n",
        "predictor = SAM2ImagePredictor(sam2_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:24:33.781651Z",
          "iopub.execute_input": "2024-10-31T04:24:33.782528Z",
          "iopub.status.idle": "2024-10-31T04:24:38.045881Z",
          "shell.execute_reply.started": "2024-10-31T04:24:33.782487Z",
          "shell.execute_reply": "2024-10-31T04:24:38.045061Z"
        },
        "trusted": true,
        "id": "x6e1Sx9X2FdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_with_prompt(image_path, save_path, model, predictor, text_prompt, box_treshold=0.35, text_treshold=0.25):\n",
        "\n",
        "    image_source, image = load_image(image_path)\n",
        "\n",
        "    boxes, logits, phrases = predict(\n",
        "        model=model,\n",
        "        image=image,\n",
        "        caption=text_prompt,\n",
        "        box_threshold=box_treshold,\n",
        "        text_threshold=text_treshold\n",
        "    )\n",
        "\n",
        "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    output_path = os.path.join(save_path, os.path.splitext(os.path.basename(image_path))[0] + '-annotated.jpg')\n",
        "\n",
        "    cv2.imwrite(output_path, annotated_frame)\n",
        "\n",
        "    h, w, _ = image_source.shape\n",
        "    boxes_unnorm = boxes * torch.Tensor([w, h, w, h])\n",
        "    boxes_xyxy = box_convert(boxes=boxes_unnorm, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    image = np.array(image.convert(\"RGB\"))\n",
        "    predictor.set_image(image)\n",
        "\n",
        "    masks, scores, _ = predictor.predict(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        box=boxes_xyxy,\n",
        "        multimask_output=False,\n",
        "    )\n",
        "\n",
        "    output_path = os.path.join(save_path, os.path.splitext(os.path.basename(image_path))[0] + '-segment.jpg')\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "    for mask in masks:\n",
        "        if len(mask.shape) > 2:\n",
        "            show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
        "        else:\n",
        "            show_mask(mask, plt.gca(), random_color=True)\n",
        "    for box in boxes_xyxy:\n",
        "        show_box(box, plt.gca())\n",
        "    plt.axis('off')\n",
        "    plt.savefig(output_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:24:45.038393Z",
          "iopub.execute_input": "2024-10-31T04:24:45.03929Z",
          "iopub.status.idle": "2024-10-31T04:24:45.051346Z",
          "shell.execute_reply.started": "2024-10-31T04:24:45.039248Z",
          "shell.execute_reply": "2024-10-31T04:24:45.050376Z"
        },
        "trusted": true,
        "id": "zr_b0E3f2FdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a directory to store all the outputs."
      ],
      "metadata": {
        "id": "DQk2SnZi2FdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /kaggle/working/output/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:24:47.466198Z",
          "iopub.execute_input": "2024-10-31T04:24:47.466603Z",
          "iopub.status.idle": "2024-10-31T04:24:48.533335Z",
          "shell.execute_reply.started": "2024-10-31T04:24:47.466564Z",
          "shell.execute_reply": "2024-10-31T04:24:48.532192Z"
        },
        "trusted": true,
        "id": "ytxhAaLg2FdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have done with defining our function, and now we can just call our function and pass necessary arguments:\n",
        "- `image_path`: Path to our interest image\n",
        "- `save_path`: Path to save the output images\n",
        "- `model`: Initialized GroundingDINO model\n",
        "- `predictor`: Initialized SAM2 predictor\n",
        "- `text_prompt`: A text prompt of our interetsed objects in the target image\n",
        "\n",
        "Let's see some examples üñºÔ∏è"
      ],
      "metadata": {
        "id": "5_4YR_gJ2FdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/box.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"kalimba . box .\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:24:52.220808Z",
          "iopub.execute_input": "2024-10-31T04:24:52.221593Z",
          "iopub.status.idle": "2024-10-31T04:25:07.309678Z",
          "shell.execute_reply.started": "2024-10-31T04:24:52.221552Z",
          "shell.execute_reply": "2024-10-31T04:25:07.308561Z"
        },
        "trusted": true,
        "id": "BIWK6SXc2FdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/cat.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"A cat\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:25:07.311308Z",
          "iopub.execute_input": "2024-10-31T04:25:07.311642Z",
          "iopub.status.idle": "2024-10-31T04:25:17.793222Z",
          "shell.execute_reply.started": "2024-10-31T04:25:07.311608Z",
          "shell.execute_reply": "2024-10-31T04:25:17.792138Z"
        },
        "trusted": true,
        "id": "yToknS4_2FdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/geese.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"Geese\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:25:26.520337Z",
          "iopub.execute_input": "2024-10-31T04:25:26.520797Z",
          "iopub.status.idle": "2024-10-31T04:25:54.398382Z",
          "shell.execute_reply.started": "2024-10-31T04:25:26.520757Z",
          "shell.execute_reply": "2024-10-31T04:25:54.397355Z"
        },
        "trusted": true,
        "id": "xwEq53xb2FdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Challenging Text Prompt <a class='anchor' id='challenging'></a> [‚Üë](#top)\n",
        "\n",
        "Let's try some more challenging text prompt to see if GroundingDINO can correctly localize our target objects and SAM2 can accurately segment the object. What I meant by \"challenging\" prompts are texts that includes descriptions (adjectives / adverbs) of an object.\n",
        "\n",
        "I have an image with a few trees, but only two of them have white flowers / leaves, the others are green and red. Let's try to detect and segment only the **white trees**."
      ],
      "metadata": {
        "id": "TYbv1REp2FdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/tree.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"Trees with white flowers\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:28:17.293715Z",
          "iopub.execute_input": "2024-10-31T04:28:17.294696Z",
          "iopub.status.idle": "2024-10-31T04:28:32.009923Z",
          "shell.execute_reply.started": "2024-10-31T04:28:17.294634Z",
          "shell.execute_reply": "2024-10-31T04:28:32.008994Z"
        },
        "trusted": true,
        "id": "9HBx5FUp2FdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">  \n",
        "Great üëç Only the white trees are segmented!\n",
        "</div>\n",
        "\n",
        "Let's try a picture with four cute panda dolls, and ask it to find panda with yellow shirt."
      ],
      "metadata": {
        "id": "GXDiR80A2FdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/panda.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"Panda in yellow shirt\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:31:59.140914Z",
          "iopub.execute_input": "2024-10-31T04:31:59.141557Z",
          "iopub.status.idle": "2024-10-31T04:32:04.539616Z",
          "shell.execute_reply.started": "2024-10-31T04:31:59.141516Z",
          "shell.execute_reply": "2024-10-31T04:32:04.5384Z"
        },
        "trusted": true,
        "id": "RUOnhuCO2FdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "Hmm... well, it detects both \"panda\" and \"yellow shirts\", which is not what I want (or am I using a wrong grammar in my prompt ü§£).\n",
        "</div>\n",
        "\n",
        "Maybe let's try another prompt using the same image. Let's segment the leftmost panda."
      ],
      "metadata": {
        "id": "ABV9PhTF2FdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/panda.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"Leftmost panda\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:38:16.558591Z",
          "iopub.execute_input": "2024-10-31T04:38:16.559012Z",
          "iopub.status.idle": "2024-10-31T04:38:21.317206Z",
          "shell.execute_reply.started": "2024-10-31T04:38:16.558974Z",
          "shell.execute_reply": "2024-10-31T04:38:21.316285Z"
        },
        "trusted": true,
        "id": "NudMEKyR2FdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "Doesn't work too... üôÅ But notice that the GroundingDINO classifies the green and orange panda as \"##most panda\" (which I believe means the left most panda) while the other two are just \"panda\".\n",
        "</div>\n",
        "\n",
        "Let's use our geese image and ask it to segment the front-most goose. As human, we can immediately tell the which direction the geese are moving towards from this image (we know which direction the geese are facing), thus we can tell which goose is walking at the front."
      ],
      "metadata": {
        "id": "SepSAxti2FdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/geese.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"Front-most goose\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:46:05.392288Z",
          "iopub.execute_input": "2024-10-31T04:46:05.392711Z",
          "iopub.status.idle": "2024-10-31T04:46:16.497773Z",
          "shell.execute_reply.started": "2024-10-31T04:46:05.392659Z",
          "shell.execute_reply": "2024-10-31T04:46:16.496721Z"
        },
        "trusted": true,
        "id": "OuytH-i22FdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">  \n",
        "<b>Suprisingly üò≤</b> it detects only a single goose, unlike the above panda example.  \n",
        "</div>\n",
        "\n",
        "But the detected goose is actually walking at the last, not the front. Perhaps the GroundingDINO misunderstood \"front-most goose\" as the goose that is nearest to the viewpoint?\n",
        "\n",
        "Maybe we can try a clearer text prompt \"goose that walks at the front\" to see what happens."
      ],
      "metadata": {
        "id": "Zw8dAbOW2FdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/geese.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"Goose that walks at the front\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:53:11.700178Z",
          "iopub.execute_input": "2024-10-31T04:53:11.701075Z",
          "iopub.status.idle": "2024-10-31T04:53:22.557085Z",
          "shell.execute_reply.started": "2024-10-31T04:53:11.701033Z",
          "shell.execute_reply": "2024-10-31T04:53:22.556149Z"
        },
        "trusted": true,
        "id": "9z0XeMlt2FdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "Same result. I think my prompt is clear enough, so I don't think it is a misunderstanding issue.\n",
        "</div>\n",
        "\n",
        "Let's try another image with mirror reflection. It is an image of a rabbit doll looking at itself through a mirror üêá Can GroundingDINO differentiate between the real rabbit and the reflected rabbit?"
      ],
      "metadata": {
        "id": "_J5Gm_MX2FdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/mirror.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"Real rabbit\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:58:01.367228Z",
          "iopub.execute_input": "2024-10-31T04:58:01.368223Z",
          "iopub.status.idle": "2024-10-31T04:58:14.026882Z",
          "shell.execute_reply.started": "2024-10-31T04:58:01.368173Z",
          "shell.execute_reply": "2024-10-31T04:58:14.025865Z"
        },
        "trusted": true,
        "id": "dMiDEfbE2FdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "It can't identify the real rabbit üôÅ\n",
        "</div>\n",
        "\n",
        "Let's change a prompt to \"rabbit inside the mirror\"."
      ],
      "metadata": {
        "id": "l_HxzWbY2FdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/mirror.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"Rabbit inside the mirror\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T04:59:24.713387Z",
          "iopub.execute_input": "2024-10-31T04:59:24.714273Z",
          "iopub.status.idle": "2024-10-31T04:59:34.838582Z",
          "shell.execute_reply.started": "2024-10-31T04:59:24.714231Z",
          "shell.execute_reply": "2024-10-31T04:59:34.837463Z"
        },
        "trusted": true,
        "id": "iqCq8yu72FdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "This prompt does not work either üôÅ\n",
        "</div>\n",
        "\n",
        "What about \"rabbit's reflection\"?"
      ],
      "metadata": {
        "id": "yngNA2Tn2FdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/kaggle/input/private-image/test_data/mirror.jpg\"\n",
        "SAVE_PATH = \"/kaggle/working/output\"\n",
        "TEXT_PROMPT = \"Rabbit's reflection\"\n",
        "\n",
        "segment_with_prompt(IMAGE_PATH, SAVE_PATH, model, predictor, TEXT_PROMPT)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T05:00:25.031987Z",
          "iopub.execute_input": "2024-10-31T05:00:25.032648Z",
          "iopub.status.idle": "2024-10-31T05:00:32.427765Z",
          "shell.execute_reply.started": "2024-10-31T05:00:25.032607Z",
          "shell.execute_reply": "2024-10-31T05:00:32.426831Z"
        },
        "trusted": true,
        "id": "g9yFfLjt2FdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">  \n",
        "Surprise üòÆ It knows which is the \"rabbit's reflection\" but does not know which is \"rabbit inside the mirror\".\n",
        "</div>\n",
        "\n",
        "Generally, GroundingDINO + SAM2 works quite well in localizing our interest objects and segment them. However, in some challenging cases or prompts, its accuracy is not guaranteed, and I personally think that prompt engineering is still required in this case to ultimately segment what we want."
      ],
      "metadata": {
        "id": "rgLPm8WK2FdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download the output images (in the `/kaggle/working/output` directory), we can zip the folder using the following command and download it manually."
      ],
      "metadata": {
        "id": "J2XRiDIv2FdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r output.zip /kaggle/working/output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T15:08:06.462648Z",
          "iopub.execute_input": "2024-10-28T15:08:06.463612Z",
          "iopub.status.idle": "2024-10-28T15:08:08.092678Z",
          "shell.execute_reply.started": "2024-10-28T15:08:06.463569Z",
          "shell.execute_reply": "2024-10-28T15:08:08.091442Z"
        },
        "trusted": true,
        "id": "D3uV2C2Q2FdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">  \n",
        "<b>Hope you enjoy learning with this notebook üòä</b>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "Q0Y5xmkm2FdR"
      }
    }
  ]
}